{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from random import randint\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('news-headlines.csv', index_col=[\"Date\"], encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the count of 'Label' column from the dataset\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot(x='Label', data=df)\n",
    "plt.xlabel('Stock Sentiments (0-Down/Same, 1-Up)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN values\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup the database\n",
    "df_backup = df.copy()\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train an test set\n",
    "train = df[df['Date'] < '20150101']\n",
    "test = df[df['Date'] > '20141231']\n",
    "print('Train size: {}, Test size: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "y_train = train['Label']\n",
    "train = train.iloc[:, 3:28]\n",
    "y_test = test['Label']\n",
    "test = test.iloc[:, 3:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and special character from the text\n",
    "train.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)\n",
    "test.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "new_columns = [str(i) for i in range(0, 24)]\n",
    "train.columns = new_columns\n",
    "test.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the entire text to lower case\n",
    "for i in new_columns:\n",
    "    train[i] = train[i].str.lower()\n",
    "    test[i] = test[i].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the columns\n",
    "train_headlines = []\n",
    "test_headlines = []\n",
    "\n",
    "for row in range(0, train.shape[0]):\n",
    "    train_headlines.append(' '.join(str(x) for x in train.iloc[row, 0:25]))\n",
    "\n",
    "for row in range(0, test.shape[0]):\n",
    "    test_headlines.append(' '.join(str(x) for x in test.iloc[row, 0:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Study\\4 - Data Science Projects\\sentiment-analysis\\sentiment-analysis.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m words \u001b[39m=\u001b[39m train_headlines[i]\u001b[39m.\u001b[39msplit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Removing the stopwords\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Stemming the words\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m words \u001b[39m=\u001b[39m [ps\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "\u001b[1;32md:\\Study\\4 - Data Science Projects\\sentiment-analysis\\sentiment-analysis.ipynb Cell 21\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m words \u001b[39m=\u001b[39m train_headlines[i]\u001b[39m.\u001b[39msplit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Removing the stopwords\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Stemming the words\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Study/4%20-%20Data%20Science%20Projects/sentiment-analysis/sentiment-analysis.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m words \u001b[39m=\u001b[39m [ps\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[39m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(_path):\n\u001b[0;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "File \u001b[1;32mc:\\Python310\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating corpus of train dataset\n",
    "ps = PorterStemmer()\n",
    "train_corpus = []\n",
    "\n",
    "for i in range(0, len(train_headlines)):\n",
    "    # Tokenizing the news-title by words\n",
    "    words = train_headlines[i].split()\n",
    "\n",
    "    # Removing the stopwords\n",
    "    words = [word for word in words if word not in set(\n",
    "        stopwords.words('english'))]\n",
    "\n",
    "    # Stemming the words\n",
    "    words = [ps.stem(word) for word in words]\n",
    "\n",
    "    # Joining the stemmed words\n",
    "    headline = ' '.join(words)\n",
    "\n",
    "    # Building a corpus of news-title\n",
    "    train_corpus.append(headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating corpus of test dataset\n",
    "test_corpus = []\n",
    "\n",
    "for i in range(0, len(test_headlines)):\n",
    "    # Tokenizing the news-title by words\n",
    "    words = test_headlines[i].split()\n",
    "\n",
    "    # Removing the stopwords\n",
    "    words = [word for word in words if word not in set(\n",
    "        stopwords.words('english'))]\n",
    "\n",
    "    # Stemming the words\n",
    "    words = [ps.stem(word) for word in words]\n",
    "\n",
    "    # Joining the stemmed words\n",
    "    headline = ' '.join(words)\n",
    "\n",
    "    # Building a corpus of news-title\n",
    "    test_corpus.append(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_words = []\n",
    "for i in list(y_train[y_train == 0].index):\n",
    "    down_words.append(train_corpus[i])\n",
    "\n",
    "up_words = []\n",
    "for i in list(y_train[y_train == 1].index):\n",
    "    up_words.append(train_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for down_words\n",
    "wordcloud1 = WordCloud(background_color='white', width=3000, height=2500).generate(down_words[1])\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Words which indicate a fall in DJIA \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for up_words\n",
    "wordcloud2 = WordCloud(background_color='white', width=3000, height=2500).generate(up_words[5])\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis('off')\n",
    "plt.title(\"Words which indicate a rise in DJIA \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000, ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform(train_corpus).toarray()\n",
    "X_test = cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision and Recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "score1 = accuracy_score(y_test, lr_y_pred)\n",
    "score2 = precision_score(y_test, lr_y_pred)\n",
    "score3 = recall_score(y_test, lr_y_pred)\n",
    "print(\"---- Scores ----\")\n",
    "print(\"Accuracy score is: {}%\".format(round(score1*100,2)))\n",
    "print(\"Precision score is: {}\".format(round(score2,2)))\n",
    "print(\"Recall score is: {}\".format(round(score3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cm = confusion_matrix(y_test, lr_y_pred)\n",
    "lr_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(data=lr_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.title('Confusion Matrix for Logistic Regression Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision and Recall\n",
    "score1 = accuracy_score(y_test, rf_y_pred)\n",
    "score2 = precision_score(y_test, rf_y_pred)\n",
    "score3 = recall_score(y_test, rf_y_pred)\n",
    "print(\"---- Scores ----\")\n",
    "print(\"Accuracy score is: {}%\".format(round(score1*100,2)))\n",
    "print(\"Precision score is: {}\".format(round(score2,2)))\n",
    "print(\"Recall score is: {}\".format(round(score3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_y_pred)\n",
    "rf_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(data=rf_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.title('Confusion Matrix for Random Forest Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 - Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "nb_y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision and Recall\n",
    "score1 = accuracy_score(y_test, nb_y_pred)\n",
    "score2 = precision_score(y_test, nb_y_pred)\n",
    "score3 = recall_score(y_test, nb_y_pred)\n",
    "print(\"---- Scores ----\")\n",
    "print(\"Accuracy score is: {}%\".format(round(score1*100,2)))\n",
    "print(\"Precision score is: {}\".format(round(score2,2)))\n",
    "print(\"Recall score is: {}\".format(round(score3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "nb_cm = confusion_matrix(y_test, nb_y_pred)\n",
    "nb_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(data=nb_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_prediction(sample_news):\n",
    "    sample_news = re.sub(pattern='[^a-zA-Z]',repl=' ', string=sample_news)\n",
    "    sample_news = sample_news.lower()\n",
    "    sample_news_words = sample_news.split()\n",
    "    sample_news_words = [word for word in sample_news_words if not word in set(stopwords.words('english'))]\n",
    "    ps = PorterStemmer()\n",
    "    final_news = [ps.stem(word) for word in sample_news_words]\n",
    "    final_news = ' '.join(final_news)\n",
    "    temp = cv.transform([final_news]).toarray()\n",
    "    return lr_classifier.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = df[df['Date'] > '20141231']\n",
    "sample_test.reset_index(inplace=True)\n",
    "sample_test = sample_test['Top1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values\n",
    "row = randint(0, sample_test.shape[0]-1)\n",
    "sample_news = sample_test[row]\n",
    "\n",
    "print('News: {}'.format(sample_news))\n",
    "if stock_prediction(sample_news):\n",
    "    print('Prediction: The stock price will remain the same or will go down.')\n",
    "else:\n",
    "    print('Prediction: The stock price will go up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values\n",
    "row = randint(0,sample_test.shape[0]-1)\n",
    "sample_news = sample_test[row]\n",
    "\n",
    "print('News: {}'.format(sample_news))\n",
    "if stock_prediction(sample_news):\n",
    "    print('Prediction: The stock price will remain the same or will go down.')\n",
    "else:\n",
    "    print('Prediction: The stock price will go up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values\n",
    "row = randint(0,sample_test.shape[0]-1)\n",
    "sample_news = sample_test[row]\n",
    "\n",
    "print('News: {}'.format(sample_news))\n",
    "if stock_prediction(sample_news):\n",
    "    print('Prediction: The stock price will remain the same or will go down.')\n",
    "else:\n",
    "    print('Prediction: The stock price will go up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values\n",
    "row = randint(0, sample_test.shape[0]-1)\n",
    "sample_news = sample_test[row]\n",
    "\n",
    "print('News: {}'.format(sample_news))\n",
    "if stock_prediction(sample_news):\n",
    "    print('Prediction: The stock price will remain the same or will go down.')\n",
    "else:\n",
    "    print('Prediction: The stock price will go up!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
